{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHQfIh6JahosGRrERVtXRx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vardhanreddy2003/GPT-2Training/blob/main/TransfomerBlock.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "ikELt7W8kxzF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_config={\n",
        "    \"vocab_size\":50257,\n",
        "    \"context_length\":1024,\n",
        "    \"emb_dim\":768,\n",
        "    \"n_heads\":12,\n",
        "    \"n_layers\":12,\n",
        "    \"dropout_rate\":0.1,\n",
        "    \"qkv_bias\":False\n",
        "}"
      ],
      "metadata": {
        "id": "INePm0GnlUhG"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class feedforwardnetwork(nn.Module):\n",
        "  def __init__(self,llm_config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feedforwardnetwork=nn.Sequential(\n",
        "        nn.Linear(llm_config[\"emb_dim\"],llm_config[\"emb_dim\"]*4),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(llm_config[\"emb_dim\"]*4,llm_config[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self,X):\n",
        "    out=self.feedforwardnetwork(X)\n",
        "    return out"
      ],
      "metadata": {
        "id": "GzxEh7sKnHnQ"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.epis=1e-5\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "  def forward(self,X):\n",
        "    self.mean=X.mean(-1,keepdim=True)\n",
        "    self.variance=X.var(-1,keepdim=True,unbiased=True)\n",
        "    norm_x=(X-self.mean)/((self.variance+self.epis)**0.5)\n",
        "\n",
        "    return self.scale*norm_x+self.shift"
      ],
      "metadata": {
        "id": "DcaUlCimrzdK"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\n",
        "            \"mask\",\n",
        "            torch.triu(torch.ones(context_length, context_length),\n",
        "                       diagonal=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec) # optional projection\n",
        "\n",
        "        return context_vec"
      ],
      "metadata": {
        "id": "LNNo8wN3qKwy"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "  def __init__(self,llm_config):\n",
        "    super().__init__()\n",
        "    self.MultiHeadAttention=MultiHeadAttention(\n",
        "         d_in=llm_config[\"emb_dim\"],\n",
        "            d_out=llm_config[\"emb_dim\"],\n",
        "            context_length=llm_config[\"context_length\"],\n",
        "            num_heads=llm_config[\"n_heads\"],\n",
        "            dropout=llm_config[\"dropout_rate\"],\n",
        "            qkv_bias=llm_config[\"qkv_bias\"]\n",
        "    )\n",
        "    self.dropout_layer=nn.Dropout(llm_config[\"dropout_rate\"])\n",
        "    self.feedforwardnetwork=feedforwardnetwork(llm_config)\n",
        "    self.layer_norm1=LayerNorm(llm_config[\"emb_dim\"])\n",
        "    self.layer_norm2=LayerNorm(llm_config[\"emb_dim\"])\n",
        "\n",
        "  def forward(self,X):\n",
        "    shortcut=X\n",
        "    X=self.layer_norm1(X)\n",
        "    X=self.MultiHeadAttention(X)\n",
        "    X=self.dropout_layer(X)\n",
        "    X=X+shortcut\n",
        "    shortcut=X\n",
        "    X=self.layer_norm2(X)\n",
        "    X=self.feedforwardnetwork(X)\n",
        "    X=self.dropout_layer(X)\n",
        "    X=X+shortcut\n",
        "    return X\n",
        "\n"
      ],
      "metadata": {
        "id": "oV7Dbe1ZqMy0"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "X=torch.rand(2,4,768)\n",
        "block=TransformerBlock(llm_config)\n",
        "out=block(X)\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B13Zr5fnz5RQ",
        "outputId": "d164a359-8b68-4827-9949-31ff3eb9ad1c"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 4, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6eaf155",
        "outputId": "45a02432-f604-4a7c-8b44-e32ed6c0cefd"
      },
      "source": [
        "print(block.feedforwardnetwork.feedforwardnetwork[0].weight)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0176,  0.0298, -0.0270,  ..., -0.0134, -0.0250, -0.0225],\n",
            "        [-0.0216,  0.0315,  0.0016,  ...,  0.0078,  0.0077, -0.0230],\n",
            "        [-0.0211,  0.0097,  0.0292,  ...,  0.0216, -0.0124,  0.0164],\n",
            "        ...,\n",
            "        [ 0.0077, -0.0191, -0.0316,  ...,  0.0225, -0.0091,  0.0247],\n",
            "        [-0.0286, -0.0322,  0.0108,  ...,  0.0288, -0.0130,  0.0138],\n",
            "        [-0.0181, -0.0186,  0.0168,  ..., -0.0075, -0.0009,  0.0138]],\n",
            "       requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}